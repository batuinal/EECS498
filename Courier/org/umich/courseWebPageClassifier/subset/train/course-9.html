<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"><title>CS195-5 Intro. to Machine Learning</title>
</head>

<body>
<center><h2>CS195-5: Introduction to Machine Learning</h2>
<h3><font color=red>Fall 2006</font></h3>
</center>

<table>
<tr>
<td align="center" valign="top" width=100>
<br><br><br>
<a href="home.html">General info</a><br><br>
Syllabus<br><br>
<a href="calendar.html">Calendar</a> <br><br>
<a href="projects.html">Projects</a> <br><br>
<a href="matlab.html">Matlab</a> <br><br>
<a href="latex.html">LaTeX</a> <br><br>
<a href="mailist.html">Mailing List</a> <br><br>
</td>
<td>

<h3>Scope and goals of the class</h3>


<p>This course introduces
the fundamental principles of statistical learning and their
implications for the basic problems of modeling and inference. The
emphasis is on algorithms and models useful in a broad range of
applications, and on understanding the underlying assumptions,
weaknesses and strengths of each approach. The ultimate goal is for the students to
become familiar with arsenal of modern machine learning, so that as
practitioners they
could judge when a particular tool is appropriate, and how it could be
adapted to the problem at hand. Developing such practical intuition is the main
desired outcome of taking the course. 
</p>

<p>
In light of this practical outlook, discussion of efficient
computation methods will receive significant attention in the
course. Part of the homework assignments
require implementing learning algorithms in
Matlab and
evaluating them on data sets, many of which will be taken from
real-life problems. The emphasis in such assignments is not on massive software
engineering but rather on algorithmic clarity and efficiency of the implementation.
</p>

<h3>Rough list of topics covered</h3>
<ul>
<li>Basic concepts in statistical learning: loss, risk, likelihood.
<li>Empirical risk minimization and generalization.
<li>Supervised learning:
<ul>
<li>Parametric regression.
<li>Additive models; logistic regression.
<li>Generative models; naive Bayes classifiers.
<li>Discriminative methods:
<ul>
<li>The perceptron and neural networks
<li>Linear discriminative analysis
<li>Support vector machines
</ul>
<li>Mixtures of experts.
</ul>
<li>Unsupervised learning:
<ul>
<li>Clustering: agglomerative, K-means, spectral.
<li>Mixture models and the EM algorithm.
<li>Dimensionality reduction.
</ul>
<li>Mutual information and entropy and their connection to learning.
<li>Feature selection.
<li>Complexity and model selection.
<li>Ensemble methods; boosting.
<li>Basics of inference and learning in graphical models.
<li>Hidden Markov models and Markov random fields.
</ul>

<h3>Helpful books</h3>
<p>
None of these books is absolutely necessary, and the lectures
and problem sets will fully contain the material required to succeed
in the course. Moreover, none of these books contains 100% of the
material covered in cs195-5. However, reading certain parts of each of these books may be
useful to understand the material; we will indicate some of these
suggested readings in the <a href="calendar.html">course calendar</a>.
</p>

<table border=0>
<tr><td><image
src="cs195home_files/bishop.jpg"></td><td>C. M. Bishop<br>Neural
Networks and Pattern Recognition<br><i>Still a great reference, but
may be superceded by the new book on the right</i></td>
<td>
<image width=100 src="cs195home_files/bishopNew.jpg"></td><td>C. M. Bishop<br>Pattern Recognition
and Machine Learning</td>
</tr>
<tr>
<td><image src="cs195home_files/Duda.jpg"></td><td>R. O. Duda,
P. E. Hart and D. G. Stork<br>Pattern Classification</td>
<td><image width=100
src="cs195home_files/mckay.jpg"></td><td>D. McKay<br>Information
Theory, Inference, and Learning Algorithms<br><i>Available <a href="http://www.inference.phy.cam.ac.uk/mackay/itila/">online</a></i></td></tr>
<tr>
<td><image width=100 src="cs195home_files/hastie.jpg"></td><td>T. Hastie, R. Tibshirani and
J. H. Friedman
<br>
The Elements of Statistical Learning
</td>
</tr>
</table>

